[
  {
    "path": "posts/2021-12-17-life-expectancy-analysis/",
    "title": "Life Expectancy Analysis",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Blaise Appolinary",
        "url": "https://example.com/norajones"
      }
    ],
    "date": "2021-12-17",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nIntroduction\r\nLoading necessary packages for the Analysis\r\nLoading Dataset from github\r\n\r\nExploratory Data Analysis\r\nData preview and cleaning\r\nVariable Corelation\r\nCategorical Variables\r\nContinuous Variables\r\n\r\n\r\nVariable Selection\r\nPreparing the data\r\nBackward Selection\r\nForward Selection\r\nLASSO Model Selection\r\nRidge Model Selection\r\nCross-Validation\r\n\r\nDiscussion\r\nResults and impacts\r\nAre the results what we expected?\r\nImprovements that could be made\r\nFuture questions/research\r\n\r\nReferences\r\n\r\nIntroduction\r\nLife expectancy is an estimate of how long a person would live on average. Throughout the years, there have been lot of studies on factors affecting life expectancy. However, these studies have not taken into account the possibility that human development index could have a significant effect. Additionally, past studies were conducted using multiple linear regressions based on world data for only one year. Therefore, there is reason to consider global data from 2000 to 2015 instead. This project relies heavily on the accuracy of the data which is provided by the Global Health Observatory (GHO) data repository under World Health Organization (WHO)\\(^{(1)}\\). Our goal is to determine the most significant variables to create an optimal linear regression model for predicting life expectancy. For instance, if current mortality rates in every age group remained constant throughout, then it would be an easily understood measure to which any person could directly relate \\(^{(2)}\\). In reality, life expectancy is affected by a variety of socioeconomic, genetic and environmental factors. Therefore, it makes for a good area of study to investigate via statistical methods to better understand the measure of life expectancy.\r\nLoading necessary packages for the Analysis\r\n\r\n\r\nShow code\r\n\r\nlibrary(hrbrthemes)\r\nlibrary(kableExtra)\r\nlibrary(ggstar)\r\nlibrary(viridis)\r\nlibrary(ggExtra)\r\nlibrary(plotly)\r\nlibrary(ggstatsplot)\r\nlibrary(patchwork)\r\nlibrary(leaps)\r\nlibrary(gganimate)\r\nlibrary(tidyverse)\r\nlibrary(tidymodels)\r\nlibrary(corrr)\r\nlibrary(mltools)\r\nlibrary(ggplot2)\r\nlibrary(elasticnet)\r\nlibrary(plotly)\r\nlibrary(caret)\r\nlibrary(tidygeocoder)\r\nlibrary(glmnet)\r\nlibrary(GGally)\r\nlibrary(DT)\r\nlibrary(broom)\r\nlibrary(corrgram)\r\noptions(repr.plot.width = 8, repr.plot.height = 8)\r\nlibrary(knitr)\r\n\r\n\r\n\r\nLoading Dataset from github\r\n\r\n\r\nShow code\r\n\r\ndata <- read.csv(\"https://raw.githubusercontent.com/zackhamza01/Life-Expectancy/main/Life%20Expectancy%20Data.csv\")\r\n\r\n\r\n\r\nExploratory Data Analysis\r\nTo begin with, we first performed some data cleaning and exploration. The exploration was very necessary for us to have a deeper understanding of the dataset before the process of gaining insights from it.\r\nData preview and cleaning\r\nTable 1\r\n\r\n\r\nShow code\r\n\r\ndata %>% head(3) %>% rmarkdown::paged_table()\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"\"],\"name\":[\"_rn_\"],\"type\":[\"\"],\"align\":[\"left\"]},{\"label\":[\"Country\"],\"name\":[1],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Year\"],\"name\":[2],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Status\"],\"name\":[3],\"type\":[\"chr\"],\"align\":[\"left\"]},{\"label\":[\"Life.expectancy\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Adult.Mortality\"],\"name\":[5],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"infant.deaths\"],\"name\":[6],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Alcohol\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"percentage.expenditure\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Hepatitis.B\"],\"name\":[9],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Measles\"],\"name\":[10],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"BMI\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"under.five.deaths\"],\"name\":[12],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Polio\"],\"name\":[13],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"Total.expenditure\"],\"name\":[14],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Diphtheria\"],\"name\":[15],\"type\":[\"int\"],\"align\":[\"right\"]},{\"label\":[\"HIV.AIDS\"],\"name\":[16],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"GDP\"],\"name\":[17],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Population\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"thinness..1.19.years\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"thinness.5.9.years\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Income.composition.of.resources\"],\"name\":[21],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"Schooling\"],\"name\":[22],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"Afghanistan\",\"2\":\"2015\",\"3\":\"Developing\",\"4\":\"65.0\",\"5\":\"263\",\"6\":\"62\",\"7\":\"0.01\",\"8\":\"71.27962\",\"9\":\"65\",\"10\":\"1154\",\"11\":\"19.1\",\"12\":\"83\",\"13\":\"6\",\"14\":\"8.16\",\"15\":\"65\",\"16\":\"0.1\",\"17\":\"584.2592\",\"18\":\"33736494\",\"19\":\"17.2\",\"20\":\"17.3\",\"21\":\"0.479\",\"22\":\"10.1\",\"_rn_\":\"1\"},{\"1\":\"Afghanistan\",\"2\":\"2014\",\"3\":\"Developing\",\"4\":\"59.9\",\"5\":\"271\",\"6\":\"64\",\"7\":\"0.01\",\"8\":\"73.52358\",\"9\":\"62\",\"10\":\"492\",\"11\":\"18.6\",\"12\":\"86\",\"13\":\"58\",\"14\":\"8.18\",\"15\":\"62\",\"16\":\"0.1\",\"17\":\"612.6965\",\"18\":\"327582\",\"19\":\"17.5\",\"20\":\"17.5\",\"21\":\"0.476\",\"22\":\"10.0\",\"_rn_\":\"2\"},{\"1\":\"Afghanistan\",\"2\":\"2013\",\"3\":\"Developing\",\"4\":\"59.9\",\"5\":\"268\",\"6\":\"66\",\"7\":\"0.01\",\"8\":\"73.21924\",\"9\":\"64\",\"10\":\"430\",\"11\":\"18.1\",\"12\":\"89\",\"13\":\"62\",\"14\":\"8.13\",\"15\":\"64\",\"16\":\"0.1\",\"17\":\"631.7450\",\"18\":\"31731688\",\"19\":\"17.7\",\"20\":\"17.7\",\"21\":\"0.470\",\"22\":\"9.9\",\"_rn_\":\"3\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nThis dataset contains variables such as country, year, status, life expectancy, etc. The Adult.Mortality column represents the adult mortality rates of both genders, which is the probability of dying between 15 and 60 years per 1000 population. Infant.deaths showcases the number of infant deaths per 1000 population. The Alcohol column describes the total litres of consumption for pure alcohol, which is recorded per capita for ages 15 and older. The Hepatitis.B, Polio, and Diphteria variables demonstrate the percentage of immunization coverage among 1-year-olds for each of those diseases. The Measles column signifies the number of reported cases of the measles per 1000 population. thinness..1.19.years` represents the percentage of thinness present in children ranging from the age of 10 to 19 years old.\r\nLife expectancy ranges from 44 to 89. This means that the people in the country with the highest life expectancy lived almost twice as long as the ones in the country with the lowest life expectancy.\r\nThe dataframe has 1649 rows and 20 attributes.\r\nVariable Corelation\r\nCategorical Variables\r\nComparison of mean life expectancy rate between developed and developing countries\r\nTo get an understanding of how the development status of a country is correlated with life expectancy, we compared the mean life expectancy of developed and developing countries.\r\nTable 2\r\n\r\n\r\nShow code\r\n\r\ndata %>%\r\n  filter(Life.expectancy>0)%>%\r\n  group_by(Status) %>%\r\n  summarize(mean_life_expectancy=mean(Life.expectancy)) %>% kbl() %>%\r\n  kable_paper(\"hover\", full_width = F)\r\n\r\n\r\n\r\nStatus\r\n\r\n\r\nmean_life_expectancy\r\n\r\n\r\nDeveloped\r\n\r\n\r\n79.19785\r\n\r\n\r\nDeveloping\r\n\r\n\r\n67.11147\r\n\r\n\r\nIn the table above, we saw that on average, people in developed countries lived almost a decade longer than people in developing countries.\r\nTo better vizualize the correlation between the development status of a country and Life expectancy, we created a boxplot as shown below.\r\n\r\n\r\nShow code\r\n\r\n#Plot showing life expectancy in different country status\r\nplt <- ggbetweenstats(\r\n  data = data,\r\n  x = Status, \r\n  y = Life.expectancy\r\n)\r\nplt <- plt + \r\n  # Add labels and title\r\n  labs(\r\n    x = \"Development Status\",\r\n    y = \"Life Expectancy\",\r\n    title = \"Life Expectancy Vs Development Status\"\r\n  ) + \r\n  # Customizations\r\n  theme(\r\n    # This is the new default font in the plot\r\n    text = element_text(family = \"Roboto\", size = 8, color = \"black\"),\r\n    plot.title = element_text(\r\n      family = \"Lobster Two\", \r\n      size = 20,\r\n      face = \"bold\",\r\n      color = \"#2a475e\"\r\n    ),\r\n    # Statistical annotations below the main title\r\n    plot.subtitle = element_text(\r\n      family = \"Roboto\", \r\n      size = 15, \r\n      face = \"bold\",\r\n      color=\"#1b2838\"\r\n    ),\r\n    plot.title.position = \"plot\", # slightly different from default\r\n    axis.text = element_text(size = 10, color = \"black\"),\r\n    axis.title = element_text(size = 12)\r\n  )+\r\n    theme(\r\n    axis.ticks = element_blank(),\r\n    axis.line = element_line(colour = \"grey50\"),\r\n    panel.grid = element_line(color = \"#b4aea9\"),\r\n    panel.grid.minor = element_blank(),\r\n    panel.grid.major.x = element_blank(),\r\n    panel.grid.major.y = element_line(linetype = \"dashed\"),\r\n    panel.background = element_rect(fill = \"#fbf9f4\", color = \"#fbf9f4\"),\r\n    plot.background = element_rect(fill = \"#fbf9f4\", color = \"#fbf9f4\")\r\n  )\r\nplt\r\n\r\n\r\n\r\n\r\nFigure 1: Box Plot showing development status and life expectancy\r\n\r\n\r\n\r\nAs we can see here, there is a stark difference in the life expectancy in developing and developed countries. To begin with, the interquartile range of life expectancy is lower in developed than developing countries.The mean and median are both much higher in developed countries. The minimum life expectancy is also much lower in developing countries. Thus, we can see that the status of the country could affect life expectancy.\r\nThe next categorical variable is Country. To show this, we created a map depicting the average life expectancy in each country.\r\n\r\n\r\nShow code\r\n\r\nnew_data <- data #%>% na.omit()\r\nsummary_data <- new_data %>% \r\n    group_by(Country) %>% \r\n    summarize(mean=mean(Life.expectancy))\r\n\r\ng <- list(\r\nshowframe = TRUE,\r\nshowcoastlines = TRUE,\r\nprojection = list(type = 'Mercator')\r\n)\r\n\r\nplot <- plot_geo(summary_data, locationmode=\"country names\")%>%\r\n    add_trace(locations=~Country,\r\n             z=~mean,\r\n             color=~mean) %>%\r\n    layout(\r\n  title = with(summary_data, paste('Mean Life Expectancy')),\r\n  geo = g\r\n)\r\nplot\r\n\r\n\r\n\r\n\r\n{\"x\":{\"visdat\":{\"30403f872ef8\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"30403f872ef8\",\"attrs\":{\"30403f872ef8\":{\"locationmode\":\"country names\",\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"locations\":{},\"z\":{},\"color\":{},\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"mapType\":\"geo\",\"title\":\"Mean Life Expectancy\",\"geo\":{\"domain\":{\"x\":[0,1],\"y\":[0,1]},\"showframe\":true,\"showcoastlines\":true,\"projection\":{\"type\":\"Mercator\"}},\"scene\":{\"zaxis\":{\"title\":\"mean\"}},\"hovermode\":\"closest\",\"showlegend\":false,\"legend\":{\"yanchor\":\"top\",\"y\":0.5}},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"colorbar\":{\"title\":\"mean\",\"ticklen\":2,\"len\":0.5,\"lenmode\":\"fraction\",\"y\":1,\"yanchor\":\"top\"},\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666666\",\"rgba(70,19,97,1)\"],[\"0.0833333333333332\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666666\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":true,\"locationmode\":\"country names\",\"locations\":[\"Afghanistan\",\"Albania\",\"Algeria\",\"Angola\",\"Antigua and Barbuda\",\"Argentina\",\"Armenia\",\"Australia\",\"Austria\",\"Azerbaijan\",\"Bahamas\",\"Bahrain\",\"Bangladesh\",\"Barbados\",\"Belarus\",\"Belgium\",\"Belize\",\"Benin\",\"Bhutan\",\"Bolivia (Plurinational State of)\",\"Bosnia and Herzegovina\",\"Botswana\",\"Brazil\",\"Brunei Darussalam\",\"Bulgaria\",\"Burkina Faso\",\"Burundi\",\"CÃ´te d'Ivoire\",\"Cabo Verde\",\"Cambodia\",\"Cameroon\",\"Canada\",\"Central African Republic\",\"Chad\",\"Chile\",\"China\",\"Colombia\",\"Comoros\",\"Congo\",\"Costa Rica\",\"Croatia\",\"Cuba\",\"Cyprus\",\"Czechia\",\"Democratic People's Republic of Korea\",\"Democratic Republic of the Congo\",\"Denmark\",\"Djibouti\",\"Dominican Republic\",\"Ecuador\",\"Egypt\",\"El Salvador\",\"Equatorial Guinea\",\"Eritrea\",\"Estonia\",\"Ethiopia\",\"Fiji\",\"Finland\",\"France\",\"Gabon\",\"Gambia\",\"Georgia\",\"Germany\",\"Ghana\",\"Greece\",\"Grenada\",\"Guatemala\",\"Guinea\",\"Guinea-Bissau\",\"Guyana\",\"Haiti\",\"Honduras\",\"Hungary\",\"Iceland\",\"India\",\"Indonesia\",\"Iran (Islamic Republic of)\",\"Iraq\",\"Ireland\",\"Israel\",\"Italy\",\"Jamaica\",\"Japan\",\"Jordan\",\"Kazakhstan\",\"Kenya\",\"Kiribati\",\"Kuwait\",\"Kyrgyzstan\",\"Lao People's Democratic Republic\",\"Latvia\",\"Lebanon\",\"Lesotho\",\"Liberia\",\"Libya\",\"Lithuania\",\"Luxembourg\",\"Madagascar\",\"Malawi\",\"Malaysia\",\"Maldives\",\"Mali\",\"Malta\",\"Mauritania\",\"Mauritius\",\"Mexico\",\"Micronesia (Federated States of)\",\"Mongolia\",\"Montenegro\",\"Morocco\",\"Mozambique\",\"Myanmar\",\"Namibia\",\"Nepal\",\"Netherlands\",\"New Zealand\",\"Nicaragua\",\"Niger\",\"Nigeria\",\"Norway\",\"Oman\",\"Pakistan\",\"Panama\",\"Papua New Guinea\",\"Paraguay\",\"Peru\",\"Philippines\",\"Poland\",\"Portugal\",\"Qatar\",\"Republic of Korea\",\"Republic of Moldova\",\"Romania\",\"Russian Federation\",\"Rwanda\",\"Saint Lucia\",\"Saint Vincent and the Grenadines\",\"Samoa\",\"Sao Tome and Principe\",\"Saudi Arabia\",\"Senegal\",\"Serbia\",\"Seychelles\",\"Sierra Leone\",\"Singapore\",\"Slovakia\",\"Slovenia\",\"Solomon Islands\",\"Somalia\",\"South Africa\",\"South Sudan\",\"Spain\",\"Sri Lanka\",\"Sudan\",\"Suriname\",\"Swaziland\",\"Sweden\",\"Switzerland\",\"Syrian Arab Republic\",\"Tajikistan\",\"Thailand\",\"The former Yugoslav republic of Macedonia\",\"Timor-Leste\",\"Togo\",\"Tonga\",\"Trinidad and Tobago\",\"Tunisia\",\"Turkey\",\"Turkmenistan\",\"Uganda\",\"Ukraine\",\"United Arab Emirates\",\"United Kingdom of Great Britain and Northern Ireland\",\"United Republic of Tanzania\",\"United States of America\",\"Uruguay\",\"Uzbekistan\",\"Vanuatu\",\"Venezuela (Bolivarian Republic of)\",\"Viet Nam\",\"Yemen\",\"Zambia\",\"Zimbabwe\"],\"z\":[58.19375,75.15625,73.61875,49.01875,75.05625,75.15625,73.4,81.8125,81.48125,70.73125,74.2875,75.725,69.3,74.35625,69.90625,80.68125,69.26875,57.56875,66.1625,67.70625,75.96875,56.05,73.38125,76.4875,72.85,55.64375,55.5375,50.3875,72.51875,64.34375,54.01875,81.6875,48.5125,50.3875,79.45,74.2625,73.2875,61.58125,59.04375,78.59375,76.11875,77.975,79.675,76.76875,69.19375,55.6875,79.25625,60.75625,72.34375,74.725,71.5,71.74375,55.3125,60.6875,74.94375,59.1125,68.7125,80.7125,82.21875,62.24375,59.4625,73.50625,81.175,60.8625,81.21875,73.29375,71.73125,56.0125,55.36875,65.6375,59.86875,72.99375,73.825,82.44375,65.41875,67.55625,73.85625,70.35625,80.15,81.3,82.1875,74.29375,82.5375,72.9875,66.7625,57.48125,65.15,73.84375,69.08125,62.38125,73.73125,74.2,48.78125,57.525,72.4875,72.80625,80.78125,62.74375,49.89375,73.75625,75.5375,54.9375,80.3625,62.8,72.7125,75.71875,68.2,65.8875,74.5,72.15625,53.39375,64.2,60.4,66.48125,81.13125,81.3375,73.45,56.98125,51.35625,81.79375,74.84375,64.5,76.4875,61.68125,73.1125,73.6625,67.575,75.65,79.99375,77.03125,80.4875,69.98125,74.05,67.7625,59.3125,73.6,73.475,73.61875,65.1625,73.46875,62.56875,73.95625,72.375,46.1125,81.475,74.75,79.73125,67.7125,53.31875,57.5,53.875,82.06875,73.4,61.825,70.08125,51.325,82.51875,82.33125,70.85,66.65625,73.08125,74.1125,64.75625,56.6625,72.53125,71.06875,74.35625,73.9125,64.61875,55.70625,69.9375,75.7,80.79375,56.00625,78.0625,76.075,68.03125,71.3875,73.3875,74.775,63.8625,53.90625,50.4875],\"type\":\"choropleth\",\"marker\":{\"line\":{\"colorbar\":{\"title\":\"\",\"ticklen\":2},\"cmin\":46.1125,\"cmax\":82.5375,\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666666\",\"rgba(70,19,97,1)\"],[\"0.0833333333333332\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666666\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"color\":[58.19375,75.15625,73.61875,49.01875,75.05625,75.15625,73.4,81.8125,81.48125,70.73125,74.2875,75.725,69.3,74.35625,69.90625,80.68125,69.26875,57.56875,66.1625,67.70625,75.96875,56.05,73.38125,76.4875,72.85,55.64375,55.5375,50.3875,72.51875,64.34375,54.01875,81.6875,48.5125,50.3875,79.45,74.2625,73.2875,61.58125,59.04375,78.59375,76.11875,77.975,79.675,76.76875,69.19375,55.6875,79.25625,60.75625,72.34375,74.725,71.5,71.74375,55.3125,60.6875,74.94375,59.1125,68.7125,80.7125,82.21875,62.24375,59.4625,73.50625,81.175,60.8625,81.21875,73.29375,71.73125,56.0125,55.36875,65.6375,59.86875,72.99375,73.825,82.44375,65.41875,67.55625,73.85625,70.35625,80.15,81.3,82.1875,74.29375,82.5375,72.9875,66.7625,57.48125,65.15,73.84375,69.08125,62.38125,73.73125,74.2,48.78125,57.525,72.4875,72.80625,80.78125,62.74375,49.89375,73.75625,75.5375,54.9375,80.3625,62.8,72.7125,75.71875,68.2,65.8875,74.5,72.15625,53.39375,64.2,60.4,66.48125,81.13125,81.3375,73.45,56.98125,51.35625,81.79375,74.84375,64.5,76.4875,61.68125,73.1125,73.6625,67.575,75.65,79.99375,77.03125,80.4875,69.98125,74.05,67.7625,59.3125,73.6,73.475,73.61875,65.1625,73.46875,62.56875,73.95625,72.375,46.1125,81.475,74.75,79.73125,67.7125,53.31875,57.5,53.875,82.06875,73.4,61.825,70.08125,51.325,82.51875,82.33125,70.85,66.65625,73.08125,74.1125,64.75625,56.6625,72.53125,71.06875,74.35625,73.9125,64.61875,55.70625,69.9375,75.7,80.79375,56.00625,78.0625,76.075,68.03125,71.3875,73.3875,74.775,63.8625,53.90625,50.4875]}},\"geo\":\"geo\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\r\nFigure 2: A geographic plot showing different\r\n\r\n\r\n\r\nShow code\r\n\r\n#htmlwidgets::saveWidget(plot, \"plot2.html\")\r\n#IRdisplay::display_html(sprintf('<iframe src=\"plot2.html\" height=%d width=%d><\/iframe>', 600, 900))\r\n\r\n\r\n\r\nHere we see that sub-saharan african countries are amongst the lowest life expectancy regions while Canada, Europe and Australia amongst the highest.\r\nContinuous Variables\r\nTo demonstrate the correlation between continuous variables and life expectancy, we created a correlation map.\r\nHIV.AIDS VS life.expectancy\r\n\r\n\r\nShow code\r\n\r\np<- data %>% \r\n  ggplot()+\r\n  geom_point(aes(HIV.AIDS, Life.expectancy, alpha=0.5,color=Status)) +\r\n  geom_smooth(aes(HIV.AIDS, Life.expectancy))+\r\n  labs(x=\"Deaths per 1000 births from AIDS\", Y=\"Life expectancy\")+\r\n  theme(legend.position = \"none\")\r\n\r\nggMarginal(p, type=\"histogram\", fill=\"slateblue\")\r\n\r\n\r\n\r\n\r\nIncome composition of resources vs life expectancy\r\n\r\n\r\nShow code\r\n\r\np2<- data %>% \r\n  ggplot()+\r\n  geom_point(aes(Income.composition.of.resources, Life.expectancy,\r\n                 color=Country,\r\n                 fill=Status,alpha=0.01, size= GDP), shape=21)+\r\n  scale_size(range=c(.1, 6))+\r\n  \r\n   scale_fill_viridis(discrete=TRUE, guide=FALSE, option=\"A\") +\r\n    theme_ipsum() +\r\n    theme(legend.position=\"bottom\") +\r\n    ylab(\"Life Expectancy\") +\r\n    xlab(\"Income Composition\") +\r\n    theme(legend.position = \"none\")\r\nggMarginal(p2, type=\"density\", fill=\"grey\")\r\n\r\n\r\n\r\n\r\nIncome composition vs Adult Mortality\r\n\r\n\r\nShow code\r\n\r\np3 <- data %>% \r\n  ggplot()+\r\n  geom_point(aes(Income.composition.of.resources, Adult.Mortality,\r\n                 color=Country,\r\n                 fill=Status,alpha=0.5, size= GDP), shape=21)+\r\n  scale_size(range=c(.1, 15))+\r\n  \r\n   scale_fill_viridis(discrete=TRUE, guide=FALSE, option=\"A\") +\r\n    theme_ipsum() +\r\n    theme(legend.position=\"bottom\") +\r\n    ylab(\"Adult Mortality\") +\r\n    xlab(\"Income Composition\") +\r\n    theme(legend.position = \"none\")\r\np3+\r\n  transition_time(Year)+\r\n  labs(title = \"Year: {frame_time}\")\r\n\r\n\r\n\r\n\r\nCorrelation matrix\r\n\r\n\r\nShow code\r\n\r\ndata_num <- data %>% select(-Country, -Year, -Status)\r\n\r\ncorrgram(data_num, \r\n         order=NULL, \r\n         lower.panel=panel.shade, \r\n         upper.panel=panel.pie, \r\n         text.panel=panel.txt)\r\n\r\n\r\n\r\n\r\n(#fig:correlation matrix)Correlation Matrix\r\n\r\n\r\n\r\nObservations\r\nThe variables that life expectancy is most positively related are: schooling, and income composition of resources.\r\nThe variables most negatively associated with life expectancy are: adult mortality and HIV/AIDS.\r\nLife expectancy has a negligible relationship with Population, Total expenditure, under-five deaths and Measles.\r\nFrom the above observations, we would expect to see schooling, income composition of resources, adult mortality, and HIV/AIDS to have the largest impact in our linear model while Population, Total expenditure, under-five deaths and Measles are expected to have a smaller impact in the linear model.\r\nVariable Selection\r\nPreparing the data\r\nOur first step is to ensure all of our variables can be used in the regression and then splitting the dataset into testing and training sets. For this study, we will be using a proportion of 0.75. We will not be using the Country and Year categories in this model as our aim is to predict the life expectancy based on factors that are more general and not specific to a place or time. Furthermore, the Country variable has over 150 levels, which will make our model extremely large.\r\n\r\n\r\nShow code\r\n\r\ndata <- data %>% \r\n    select(-Country) %>%\r\n    select(-Year) %>%\r\n    mutate(Status = if_else(Status == \"Developing\", 0, 1)) %>%\r\n    na.omit()\r\ndata$Status <- as.factor(data$Status)\r\n\r\n#splitting data into testing and training sets\r\nset.seed(123)\r\ninitial_split <- initial_split(data, prop = 0.75)\r\nexpectancy_train <- training(initial_split)\r\nexpectancy_test <- testing(initial_split)\r\n\r\n\r\nexpectancy_train %>% head(2) %>% kbl(caption = \"Table 3: First few contents of the training set\") %>%\r\n  kable_paper(\"hover\", full_width = F)\r\n\r\n\r\n\r\nTable 1: Table 3: First few contents of the training set\r\n\r\n\r\n\r\n\r\nStatus\r\n\r\n\r\nLife.expectancy\r\n\r\n\r\nAdult.Mortality\r\n\r\n\r\ninfant.deaths\r\n\r\n\r\nAlcohol\r\n\r\n\r\npercentage.expenditure\r\n\r\n\r\nHepatitis.B\r\n\r\n\r\nMeasles\r\n\r\n\r\nBMI\r\n\r\n\r\nunder.five.deaths\r\n\r\n\r\nPolio\r\n\r\n\r\nTotal.expenditure\r\n\r\n\r\nDiphtheria\r\n\r\n\r\nHIV.AIDS\r\n\r\n\r\nGDP\r\n\r\n\r\nPopulation\r\n\r\n\r\nthinness..1.19.years\r\n\r\n\r\nthinness.5.9.years\r\n\r\n\r\nIncome.composition.of.resources\r\n\r\n\r\nSchooling\r\n\r\n\r\n676\r\n\r\n\r\n1\r\n\r\n\r\n81.0\r\n\r\n\r\n54\r\n\r\n\r\n0\r\n\r\n\r\n9.04\r\n\r\n\r\n212.08593\r\n\r\n\r\n96\r\n\r\n\r\n0\r\n\r\n\r\n59.2\r\n\r\n\r\n0\r\n\r\n\r\n99\r\n\r\n\r\n7.46\r\n\r\n\r\n99\r\n\r\n\r\n0.1\r\n\r\n\r\n2797.967\r\n\r\n\r\n1143896\r\n\r\n\r\n0.9\r\n\r\n\r\n1.0\r\n\r\n\r\n0.850\r\n\r\n\r\n13.8\r\n\r\n\r\n799\r\n\r\n\r\n0\r\n\r\n\r\n74.4\r\n\r\n\r\n151\r\n\r\n\r\n8\r\n\r\n\r\n3.69\r\n\r\n\r\n18.26183\r\n\r\n\r\n82\r\n\r\n\r\n0\r\n\r\n\r\n45.8\r\n\r\n\r\n10\r\n\r\n\r\n97\r\n\r\n\r\n6.46\r\n\r\n\r\n87\r\n\r\n\r\n0.3\r\n\r\n\r\n244.469\r\n\r\n\r\n1328961\r\n\r\n\r\n1.5\r\n\r\n\r\n1.4\r\n\r\n\r\n0.679\r\n\r\n\r\n12.6\r\n\r\n\r\nShow code\r\n\r\nexpectancy_test %>% head(2) %>% kbl(caption = \"Table 4: First few contents of the test set\") %>%\r\n  kable_paper(\"hover\", full_width = F)\r\n\r\n\r\n\r\nTable 1: Table 4: First few contents of the test set\r\n\r\n\r\n\r\n\r\nStatus\r\n\r\n\r\nLife.expectancy\r\n\r\n\r\nAdult.Mortality\r\n\r\n\r\ninfant.deaths\r\n\r\n\r\nAlcohol\r\n\r\n\r\npercentage.expenditure\r\n\r\n\r\nHepatitis.B\r\n\r\n\r\nMeasles\r\n\r\n\r\nBMI\r\n\r\n\r\nunder.five.deaths\r\n\r\n\r\nPolio\r\n\r\n\r\nTotal.expenditure\r\n\r\n\r\nDiphtheria\r\n\r\n\r\nHIV.AIDS\r\n\r\n\r\nGDP\r\n\r\n\r\nPopulation\r\n\r\n\r\nthinness..1.19.years\r\n\r\n\r\nthinness.5.9.years\r\n\r\n\r\nIncome.composition.of.resources\r\n\r\n\r\nSchooling\r\n\r\n\r\n3\r\n\r\n\r\n0\r\n\r\n\r\n59.9\r\n\r\n\r\n268\r\n\r\n\r\n66\r\n\r\n\r\n0.01\r\n\r\n\r\n73.21924\r\n\r\n\r\n64\r\n\r\n\r\n430\r\n\r\n\r\n18.1\r\n\r\n\r\n89\r\n\r\n\r\n62\r\n\r\n\r\n8.13\r\n\r\n\r\n64\r\n\r\n\r\n0.1\r\n\r\n\r\n631.7450\r\n\r\n\r\n31731688\r\n\r\n\r\n17.7\r\n\r\n\r\n17.7\r\n\r\n\r\n0.470\r\n\r\n\r\n9.9\r\n\r\n\r\n7\r\n\r\n\r\n0\r\n\r\n\r\n58.6\r\n\r\n\r\n281\r\n\r\n\r\n77\r\n\r\n\r\n0.01\r\n\r\n\r\n56.76222\r\n\r\n\r\n63\r\n\r\n\r\n2861\r\n\r\n\r\n16.2\r\n\r\n\r\n106\r\n\r\n\r\n63\r\n\r\n\r\n9.42\r\n\r\n\r\n63\r\n\r\n\r\n0.1\r\n\r\n\r\n445.8933\r\n\r\n\r\n284331\r\n\r\n\r\n18.6\r\n\r\n\r\n18.7\r\n\r\n\r\n0.434\r\n\r\n\r\n8.9\r\n\r\n\r\nBackward Selection\r\nWe will now be performing variable selection using regsubsets. Our nvmax is 19 as there are 19 remaining variables in the dataset.\r\n\r\n\r\nShow code\r\n\r\n#Variable selection \r\n\r\nbackward_sel <- regsubsets(x=Life.expectancy ~.,data= expectancy_train, nvmax=19, method=\"backward\")\r\n\r\n#backward_sel\r\nback_summary <- summary(backward_sel)\r\n\r\nback_summary_df <- data.frame( \r\n   RSQ = back_summary$rsq,\r\n   RSS = back_summary$rss,\r\n   ADJ.R2 = back_summary$adjr2,\r\n   BIC = back_summary$bic\r\n)\r\nback_summary_df %>% head() %>% kbl(caption = \"Backward selection\") %>%\r\n  kable_paper(\"hover\", full_width = F)\r\n\r\n\r\n\r\nTable 2: Backward selection\r\n\r\n\r\nRSQ\r\n\r\n\r\nRSS\r\n\r\n\r\nADJ.R2\r\n\r\n\r\nBIC\r\n\r\n\r\n0.5177821\r\n\r\n\r\n46475.58\r\n\r\n\r\n0.5173913\r\n\r\n\r\n-887.2486\r\n\r\n\r\n0.7279422\r\n\r\n\r\n26220.60\r\n\r\n\r\n0.7275009\r\n\r\n\r\n-1587.5926\r\n\r\n\r\n0.7898490\r\n\r\n\r\n20254.10\r\n\r\n\r\n0.7893372\r\n\r\n\r\n-1899.5934\r\n\r\n\r\n0.8105975\r\n\r\n\r\n18254.38\r\n\r\n\r\n0.8099821\r\n\r\n\r\n-2020.9589\r\n\r\n\r\n0.8145800\r\n\r\n\r\n17870.55\r\n\r\n\r\n0.8138263\r\n\r\n\r\n-2040.1052\r\n\r\n\r\n0.8249509\r\n\r\n\r\n16871.02\r\n\r\n\r\n0.8240963\r\n\r\n\r\n-2104.1260\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\n## Using Mallow's C_P to find the right number of inputs\r\nbackward_sel_candidate_models <- tibble(\r\n  n_input_variables = rep(1:19),\r\n  C_p =back_summary$cp\r\n)\r\ncat(\"The best number of predictors to be selected is \", backward_sel_candidate_models %>% filter(C_p==min(C_p)) %>% pull(n_input_variables))\r\n\r\n\r\nThe best number of predictors to be selected is  12\r\n\r\nShow code\r\n\r\ncat(\"\\nTable 6\")\r\n\r\n\r\n\r\nTable 6\r\n\r\nShow code\r\n\r\nbackward_sel_candidate_models %>% kbl(caption = \"Table 6\") %>%\r\n  kable_paper(\"hover\", full_width = F) %>% \r\n  row_spec(12, bold = T, color = \"white\", background = \"brown\")\r\n\r\n\r\n\r\nTable 3: Table 6\r\n\r\n\r\nn_input_variables\r\n\r\n\r\nC_p\r\n\r\n\r\n1\r\n\r\n\r\n2404.139354\r\n\r\n\r\n2\r\n\r\n\r\n821.437844\r\n\r\n\r\n3\r\n\r\n\r\n356.633125\r\n\r\n\r\n4\r\n\r\n\r\n202.179470\r\n\r\n\r\n5\r\n\r\n\r\n174.149690\r\n\r\n\r\n6\r\n\r\n\r\n97.948598\r\n\r\n\r\n7\r\n\r\n\r\n50.986642\r\n\r\n\r\n8\r\n\r\n\r\n19.934640\r\n\r\n\r\n9\r\n\r\n\r\n13.454595\r\n\r\n\r\n10\r\n\r\n\r\n11.985773\r\n\r\n\r\n11\r\n\r\n\r\n11.468809\r\n\r\n\r\n12\r\n\r\n\r\n9.988715\r\n\r\n\r\n13\r\n\r\n\r\n10.121462\r\n\r\n\r\n14\r\n\r\n\r\n11.148805\r\n\r\n\r\n15\r\n\r\n\r\n12.691486\r\n\r\n\r\n16\r\n\r\n\r\n14.287013\r\n\r\n\r\n17\r\n\r\n\r\n16.079369\r\n\r\n\r\n18\r\n\r\n\r\n18.017771\r\n\r\n\r\n19\r\n\r\n\r\n20.000000\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nplot(back_summary$cp,\r\n    main = \"Cp for Backward Method using a Training Set\",\r\n  xlab = \"Number of Input Variables\", ylab = \"Cp\", type = \"b\", pch = 19,\r\n  col = \"red\")\r\n\r\n\r\n\r\n\r\nThe results of our backward selection tell us that the model with 12 variables is the best model, as it has the lowest \\(C_p\\) of 9.988715.\r\n\r\n\r\nShow code\r\n\r\nback_summary %>% head(1) %>% kbl() %>%\r\n  kable_paper(\"hover\", full_width = F)\r\n\r\n\r\n\r\n\r\n(Intercept)\r\n\r\n\r\nStatus1\r\n\r\n\r\nAdult.Mortality\r\n\r\n\r\ninfant.deaths\r\n\r\n\r\nAlcohol\r\n\r\n\r\npercentage.expenditure\r\n\r\n\r\nHepatitis.B\r\n\r\n\r\nMeasles\r\n\r\n\r\nBMI\r\n\r\n\r\nunder.five.deaths\r\n\r\n\r\nPolio\r\n\r\n\r\nTotal.expenditure\r\n\r\n\r\nDiphtheria\r\n\r\n\r\nHIV.AIDS\r\n\r\n\r\nGDP\r\n\r\n\r\nPopulation\r\n\r\n\r\nthinness..1.19.years\r\n\r\n\r\nthinness.5.9.years\r\n\r\n\r\nIncome.composition.of.resources\r\n\r\n\r\nSchooling\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nFALSE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\nTRUE\r\n\r\n\r\n\r\nThe 12 variables include: Status, Adult.Mortality, infant.deaths, percentage.expenditure, BMI, under.five.deaths, Diphtheria, HIV.AIDS, thinness.5.9.years, Income.composition.of.resources, Schooling and Alcohol.\r\nObservations\r\nPolio and Hepatitis B are not included in this model while Diphtheria is. This suggests that Diphtheria affects life expectancy more than the other two diseases, which could be due to the fact that Diphtheria is more fatal in children (fatality rate is 5-10%, while Polio’s is 2-5% and Hepatitis B’ s is around 0.42 per 100000 population).\r\nThinness 5-9 years is selected, but not thinness 10-19 years. This could be due to the fact that thinness and malnutrition during childhood may lead to people dying earlier, and so it has a larger impact on life expectancy and was thus included in the model.\r\nWhile Status and percentage expenditure both impact life expectancy, GDP and total expenditure do not. This makes sense as GDP and total expenditure are both absolute measures, and do not consider population of a country, while percentage expenditure takes into account the differences between population and cost of living in different areas.\r\nForward Selection\r\nUsing the same methods, we will now be performing forward selection.\r\nTable 7\r\n\r\n\r\nShow code\r\n\r\n### Forward selection\r\n\r\nforward_sel <- regsubsets(x=Life.expectancy ~.,data= expectancy_train, nvmax=19, method=\"forward\")\r\n\r\nforward_summary <- summary(forward_sel)\r\n\r\nforward_summary_df <- data.frame( \r\n   RSQ = forward_summary$rsq,\r\n   RSS = forward_summary$rss,\r\n   ADJ.R2 = forward_summary$adjr2,\r\n   BIC = forward_summary$bic\r\n)\r\nforward_summary_df %>% head() %>% kbl() %>% kable_paper(\"hover\", full_width=0)\r\n\r\n\r\n\r\nRSQ\r\n\r\n\r\nRSS\r\n\r\n\r\nADJ.R2\r\n\r\n\r\nBIC\r\n\r\n\r\n0.5177821\r\n\r\n\r\n46475.58\r\n\r\n\r\n0.5173913\r\n\r\n\r\n-887.2486\r\n\r\n\r\n0.7279422\r\n\r\n\r\n26220.60\r\n\r\n\r\n0.7275009\r\n\r\n\r\n-1587.5926\r\n\r\n\r\n0.7898490\r\n\r\n\r\n20254.10\r\n\r\n\r\n0.7893372\r\n\r\n\r\n-1899.5934\r\n\r\n\r\n0.8105975\r\n\r\n\r\n18254.38\r\n\r\n\r\n0.8099821\r\n\r\n\r\n-2020.9589\r\n\r\n\r\n0.8161367\r\n\r\n\r\n17720.52\r\n\r\n\r\n0.8153893\r\n\r\n\r\n-2050.5256\r\n\r\n\r\n0.8216015\r\n\r\n\r\n17193.83\r\n\r\n\r\n0.8207306\r\n\r\n\r\n-2080.6997\r\n\r\n\r\nThe number of inputs with the minimal C_P is 12\r\n\r\n\r\nShow code\r\n\r\n## Using Mallow's C_P to find the right number of inputs\r\nforward_sel_candidate_models <- tibble(\r\n  n_input_variables = rep(1:19),\r\n  C_p = forward_summary$cp\r\n)\r\n\r\nforward_sel_candidate_models %>% kbl %>% kable_paper(\"hover\", full_width=0) %>% row_spec(12, bold = T, color = \"white\", background = \"brown\")\r\n\r\n\r\n\r\nn_input_variables\r\n\r\n\r\nC_p\r\n\r\n\r\n1\r\n\r\n\r\n2404.139354\r\n\r\n\r\n2\r\n\r\n\r\n821.437844\r\n\r\n\r\n3\r\n\r\n\r\n356.633125\r\n\r\n\r\n4\r\n\r\n\r\n202.179470\r\n\r\n\r\n5\r\n\r\n\r\n162.411842\r\n\r\n\r\n6\r\n\r\n\r\n123.204489\r\n\r\n\r\n7\r\n\r\n\r\n106.087435\r\n\r\n\r\n8\r\n\r\n\r\n19.934640\r\n\r\n\r\n9\r\n\r\n\r\n13.454595\r\n\r\n\r\n10\r\n\r\n\r\n11.985773\r\n\r\n\r\n11\r\n\r\n\r\n11.468809\r\n\r\n\r\n12\r\n\r\n\r\n9.988715\r\n\r\n\r\n13\r\n\r\n\r\n10.121462\r\n\r\n\r\n14\r\n\r\n\r\n11.148805\r\n\r\n\r\n15\r\n\r\n\r\n12.691486\r\n\r\n\r\n16\r\n\r\n\r\n14.287013\r\n\r\n\r\n17\r\n\r\n\r\n16.079369\r\n\r\n\r\n18\r\n\r\n\r\n18.017771\r\n\r\n\r\n19\r\n\r\n\r\n20.000000\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nplot(forward_summary$cp,\r\n    main = \"Cp for Forward Method using a Training Set\",\r\n  xlab = \"Number of Input Variables\", ylab = \"Cp\", type = \"b\", pch = 19,\r\n  col = \"red\")\r\n\r\n\r\n\r\n\r\nThe model size we get is the same for both forward and backward selection, and contains the same variables. Due to multicollinearity, we will be removing the Schooling and infant.deaths from our model. The covariates we will be using are listed below as stepwise_selection.\r\n\r\n\r\nShow code\r\n\r\nstepwise_selection <- data %>%\r\n    select( -Hepatitis.B, -Measles, -Polio, -Total.expenditure, -GDP, \r\n           -Population, -thinness..1.19.years, -Schooling, -infant.deaths) %>%\r\n    names() \r\nstepwise_selection %>% kbl() %>% kable_paper(\"hover\", full_width=0)\r\n\r\n\r\n\r\nx\r\n\r\n\r\nStatus\r\n\r\n\r\nLife.expectancy\r\n\r\n\r\nAdult.Mortality\r\n\r\n\r\nAlcohol\r\n\r\n\r\npercentage.expenditure\r\n\r\n\r\nBMI\r\n\r\n\r\nunder.five.deaths\r\n\r\n\r\nDiphtheria\r\n\r\n\r\nHIV.AIDS\r\n\r\n\r\nthinness.5.9.years\r\n\r\n\r\nIncome.composition.of.resources\r\n\r\n\r\nBelow, we can find our model, and the summary of our model.\r\n\r\n\r\nShow code\r\n\r\nMLR_expectancy <- lm(Life.expectancy ~ Status + Adult.Mortality + percentage.expenditure\r\n                     + BMI + under.five.deaths + Diphtheria + Alcohol + \r\n                     HIV.AIDS + thinness.5.9.years + Income.composition.of.resources, data = expectancy_train)\r\ntidy(MLR_expectancy, conf.int = TRUE) %>% kbl() %>% \r\n  kable_paper(\"hover\", full_width=0)\r\n\r\n\r\n\r\nterm\r\n\r\n\r\nestimate\r\n\r\n\r\nstd.error\r\n\r\n\r\nstatistic\r\n\r\n\r\np.value\r\n\r\n\r\nconf.low\r\n\r\n\r\nconf.high\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n58.5126455\r\n\r\n\r\n0.7554403\r\n\r\n\r\n77.4550272\r\n\r\n\r\n0.0000000\r\n\r\n\r\n57.0305454\r\n\r\n\r\n59.9947456\r\n\r\n\r\nStatus1\r\n\r\n\r\n1.2412777\r\n\r\n\r\n0.4297390\r\n\r\n\r\n2.8884455\r\n\r\n\r\n0.0039396\r\n\r\n\r\n0.3981717\r\n\r\n\r\n2.0843837\r\n\r\n\r\nAdult.Mortality\r\n\r\n\r\n-0.0206131\r\n\r\n\r\n0.0011753\r\n\r\n\r\n-17.5392368\r\n\r\n\r\n0.0000000\r\n\r\n\r\n-0.0229188\r\n\r\n\r\n-0.0183073\r\n\r\n\r\npercentage.expenditure\r\n\r\n\r\n0.0004885\r\n\r\n\r\n0.0000771\r\n\r\n\r\n6.3324696\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0003371\r\n\r\n\r\n0.0006398\r\n\r\n\r\nBMI\r\n\r\n\r\n0.0570020\r\n\r\n\r\n0.0074777\r\n\r\n\r\n7.6229339\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0423315\r\n\r\n\r\n0.0716725\r\n\r\n\r\nunder.five.deaths\r\n\r\n\r\n-0.0027101\r\n\r\n\r\n0.0007291\r\n\r\n\r\n-3.7169385\r\n\r\n\r\n0.0002108\r\n\r\n\r\n-0.0041405\r\n\r\n\r\n-0.0012796\r\n\r\n\r\nDiphtheria\r\n\r\n\r\n0.0274355\r\n\r\n\r\n0.0054988\r\n\r\n\r\n4.9893188\r\n\r\n\r\n0.0000007\r\n\r\n\r\n0.0166473\r\n\r\n\r\n0.0382236\r\n\r\n\r\nAlcohol\r\n\r\n\r\n-0.0119317\r\n\r\n\r\n0.0395812\r\n\r\n\r\n-0.3014486\r\n\r\n\r\n0.7631236\r\n\r\n\r\n-0.0895861\r\n\r\n\r\n0.0657227\r\n\r\n\r\nHIV.AIDS\r\n\r\n\r\n-0.4122371\r\n\r\n\r\n0.0211421\r\n\r\n\r\n-19.4983807\r\n\r\n\r\n0.0000000\r\n\r\n\r\n-0.4537159\r\n\r\n\r\n-0.3707584\r\n\r\n\r\nthinness.5.9.years\r\n\r\n\r\n-0.0340371\r\n\r\n\r\n0.0330401\r\n\r\n\r\n-1.0301765\r\n\r\n\r\n0.3031305\r\n\r\n\r\n-0.0988585\r\n\r\n\r\n0.0307843\r\n\r\n\r\nIncome.composition.of.resources\r\n\r\n\r\n16.5937484\r\n\r\n\r\n0.8900820\r\n\r\n\r\n18.6429444\r\n\r\n\r\n0.0000000\r\n\r\n\r\n14.8474945\r\n\r\n\r\n18.3400023\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nMLR_expectancy_2 <- lm(Life.expectancy ~ Status + Adult.Mortality + percentage.expenditure\r\n                    + BMI + under.five.deaths + Diphtheria + Alcohol + \r\n                    HIV.AIDS + thinness.5.9.years + Income.composition.of.resources + \r\n                      infant.deaths + Schooling, data = expectancy_train)\r\ntidy(MLR_expectancy_2, conf.int=TRUE) %>% kbl() %>% \r\n  kable_paper(\"hover\", full_width=0)\r\n\r\n\r\n\r\nterm\r\n\r\n\r\nestimate\r\n\r\n\r\nstd.error\r\n\r\n\r\nstatistic\r\n\r\n\r\np.value\r\n\r\n\r\nconf.low\r\n\r\n\r\nconf.high\r\n\r\n\r\n(Intercept)\r\n\r\n\r\n54.6706112\r\n\r\n\r\n0.7911543\r\n\r\n\r\n69.102340\r\n\r\n\r\n0.0000000\r\n\r\n\r\n53.1184413\r\n\r\n\r\n56.2227812\r\n\r\n\r\nStatus1\r\n\r\n\r\n0.7342522\r\n\r\n\r\n0.3916697\r\n\r\n\r\n1.874672\r\n\r\n\r\n0.0610761\r\n\r\n\r\n-0.0341668\r\n\r\n\r\n1.5026712\r\n\r\n\r\nAdult.Mortality\r\n\r\n\r\n-0.0177494\r\n\r\n\r\n0.0010812\r\n\r\n\r\n-16.415941\r\n\r\n\r\n0.0000000\r\n\r\n\r\n-0.0198707\r\n\r\n\r\n-0.0156281\r\n\r\n\r\npercentage.expenditure\r\n\r\n\r\n0.0004508\r\n\r\n\r\n0.0000701\r\n\r\n\r\n6.431575\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0003133\r\n\r\n\r\n0.0005883\r\n\r\n\r\nBMI\r\n\r\n\r\n0.0339693\r\n\r\n\r\n0.0069625\r\n\r\n\r\n4.878866\r\n\r\n\r\n0.0000012\r\n\r\n\r\n0.0203095\r\n\r\n\r\n0.0476292\r\n\r\n\r\nunder.five.deaths\r\n\r\n\r\n-0.0749374\r\n\r\n\r\n0.0084056\r\n\r\n\r\n-8.915167\r\n\r\n\r\n0.0000000\r\n\r\n\r\n-0.0914284\r\n\r\n\r\n-0.0584464\r\n\r\n\r\nDiphtheria\r\n\r\n\r\n0.0128051\r\n\r\n\r\n0.0050786\r\n\r\n\r\n2.521364\r\n\r\n\r\n0.0118163\r\n\r\n\r\n0.0028413\r\n\r\n\r\n0.0227689\r\n\r\n\r\nAlcohol\r\n\r\n\r\n-0.0814556\r\n\r\n\r\n0.0375585\r\n\r\n\r\n-2.168764\r\n\r\n\r\n0.0302928\r\n\r\n\r\n-0.1551419\r\n\r\n\r\n-0.0077693\r\n\r\n\r\nHIV.AIDS\r\n\r\n\r\n-0.4153210\r\n\r\n\r\n0.0191870\r\n\r\n\r\n-21.645997\r\n\r\n\r\n0.0000000\r\n\r\n\r\n-0.4529641\r\n\r\n\r\n-0.3776780\r\n\r\n\r\nthinness.5.9.years\r\n\r\n\r\n-0.0626015\r\n\r\n\r\n0.0301367\r\n\r\n\r\n-2.077252\r\n\r\n\r\n0.0379866\r\n\r\n\r\n-0.1217268\r\n\r\n\r\n-0.0034762\r\n\r\n\r\nIncome.composition.of.resources\r\n\r\n\r\n9.2798242\r\n\r\n\r\n0.9431914\r\n\r\n\r\n9.838750\r\n\r\n\r\n0.0000000\r\n\r\n\r\n7.4293717\r\n\r\n\r\n11.1302768\r\n\r\n\r\ninfant.deaths\r\n\r\n\r\n0.0986923\r\n\r\n\r\n0.0113129\r\n\r\n\r\n8.723857\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.0764974\r\n\r\n\r\n0.1208871\r\n\r\n\r\nSchooling\r\n\r\n\r\n0.8770596\r\n\r\n\r\n0.0667411\r\n\r\n\r\n13.141222\r\n\r\n\r\n0.0000000\r\n\r\n\r\n0.7461199\r\n\r\n\r\n1.0079994\r\n\r\n\r\nAnalysing The Effect Of Multicollinearity\r\nFrom the correlation matrix above, we know that the pair of Schooling and Income.composition.of.resources as well as the pair under.five.deaths and infant.deaths are highly correlated. The first model MLR_expectancy adjusts for this by removing one variable each from both correlated pairs while MLR_expectancy_2 does not. The impact of not adjusting the model is a decrease in the magnitude of the coefficient estimate which is caused by the newly added correlated variable taking away some of its significance. In our models: \\(\\\\\\\\\\) Income.composition.of.resources : 16.6 (without) Income.composition.of.resources: 9.28 (with)under.five.deaths: -2.710e-03 (without)under.five.deaths: -7.494e-02 (with) Therefore, it is not ideal to include correlated covariates Schooling and infant.deaths in the model.\r\nBelow, we also do forward selection for interactive models, which we put a nvmax of 189 as those are the maximum number of interactions we can get.\r\n\r\n\r\nShow code\r\n\r\nforward_int_sel <- regsubsets(x=Life.expectancy ~. ^2,data= expectancy_train, nvmax = 189, method=\"forward\")\r\n\r\n\r\nReordering variables and trying again:\r\n\r\nShow code\r\n\r\nforward_int_summary <- summary(forward_int_sel)\r\n\r\nforward_int_sel_candidate_models <- tibble(\r\n  n_input_variables = rep(1:length(forward_int_summary$cp)),\r\n  C_p = forward_int_summary$cp\r\n)\r\nmin(forward_int_summary$cp) %>% kbl(caption = \"Min C_p\") %>% \r\n  kable_paper(\"hover\", full_width=0)\r\n\r\n\r\n\r\nTable 4: Min C_p\r\n\r\n\r\nx\r\n\r\n\r\n105.696\r\n\r\n\r\nAs shown, the lowest \\(C_p\\) for the forward selection models with interaction are much higher than the ones for the additive models. This suggests that the models with interaction may not be as good as that without, and as there are an incredibly large number of variables, we will not be doing any interactive models.\r\nLASSO Model Selection\r\nWe will now be using LASSO to further see what model is the best when all the covariates are included. Below, we use cv.glmnet to find the smallest lambda to further create a model and do analysis.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\ncv_lambda_LASSO <- cv.glmnet(\r\n  x = as.matrix(expectancy_train[,-c(1,2)]), y = as.matrix(expectancy_train[,2]),\r\n  alpha = 1,\r\n  lambda = exp(seq(-8, 3, 0.2)),\r\n  family = 'poisson'\r\n)\r\nlambda_min_MSE_LASSO <- round(cv_lambda_LASSO$lambda.min, 5)\r\n\r\nplot(cv_lambda_LASSO, main = \"Lambda selection by CV with LASSO\\n\\n\")\r\n\r\n\r\n\r\n\r\nFigure 3: TRUE\r\n\r\n\r\n\r\nShow code\r\n\r\nplot(cv_lambda_LASSO$glmnet.fit, \"lambda\")\r\nabline(v = log(lambda_min_MSE_LASSO), col = \"red\", lwd = 3, lty = 2)\r\n\r\n\r\n\r\n\r\nFigure 4: TRUE\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nlambda_min_MSE_LASSO %>% kbl() %>% \r\n  kable_paper(\"hover\", full_width=0)\r\n\r\n\r\n\r\nx\r\n\r\n\r\n0.00136\r\n\r\n\r\nThe smallest lambda we found from LASSO is 0.00136.\r\nRidge Model Selection\r\nWe will also be using ridge as a model selection method. Below, we perform ridge and find the smallest lambda for our models.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\n\r\ncv_lambda_RIDGE <- cv.glmnet(\r\n  x = as.matrix(expectancy_train[,-c(1,2)]), y = as.matrix(expectancy_train[,2]),\r\n  alpha = 0,\r\n  lambda = exp(seq(-12, 10, 0.3)),\r\n  family = 'poisson'\r\n)\r\nlambda_min_MSE_RIDGE <- round(cv_lambda_RIDGE$lambda.min, 5)\r\nplot(cv_lambda_RIDGE, main = \"Lambda selection by CV with RIDGE\\n\\n\")\r\n\r\n\r\n\r\n\r\nFigure 5: TRUE\r\n\r\n\r\n\r\nShow code\r\n\r\nplot(cv_lambda_RIDGE$glmnet.fit, \"lambda\")\r\n\r\nabline(v = log(lambda_min_MSE_RIDGE), col = \"red\", lwd = 3, lty = 2)\r\n\r\n\r\n\r\n\r\nFigure 6: TRUE\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nlambda_min_MSE_RIDGE\r\n\r\n\r\n[1] 0.00335\r\n\r\nCross-Validation\r\nWe will now be performing cross-validation to test our predictive model. Below, we have a few different models. - model_5CV and model_holdout are the cross-validation with 5 folds and holdout models respectively built using the stepwise covariates found through the variable selection methods. - model_lasso_cv and model_lasso_holdout are the cross validation with 5 fold and holdout models built using lasso for the cross validation and the lambda found from LASSO for the holdout model. - model_ridge_cv and model_ridge_holdout are the cross validation with 5 fold and holdout models built using ridge for the cross validation and the lambda found from ridge for the holdout model. - Lastly, model_elastic is the cross validation done for 5-fold using the elastic method.\r\n\r\n\r\nShow code\r\n\r\nset.seed(123)\r\nlambda <- 10^seq(-3, 3, length = 100)\r\n# define training control\r\ntrain_control <- trainControl(method=\"cv\", number = 5)\r\n\r\n# train the models\r\n#5 fold CV using stepwise covariates\r\nmodel_5CV <- train(Life.expectancy~., data = expectancy_train[,stepwise_selection], \r\n               trControl=train_control, method=\"lm\", preProcess = c(\"center\", \"scale\"))\r\n\r\n#stepwise covariates lm holdout\r\nmodel_holdout <- lm(Life.expectancy~., data = expectancy_train[,stepwise_selection]) \r\n\r\n#lasso with CV\r\nmodel_lasso_cv <- train(Life.expectancy~., data = expectancy_train[,-1], \r\n               trControl=train_control, method=\"glmnet\", preProcess = c(\"center\", \"scale\"),tuneGrid = expand.grid(alpha = 1, lambda = lambda))\r\n\r\n#ridge with CV\r\nmodel_ridge_cv <- train(Life.expectancy~., data = expectancy_train[,-1], \r\n               trControl=train_control, method=\"glmnet\", preProcess = c(\"center\", \"scale\"),tuneGrid = expand.grid(alpha = 0, lambda = lambda))\r\n\r\n#lasso holdout\r\nmodel_lasso_holdout <- glmnet( \r\n  x = as.matrix(expectancy_train[,-c(1,2)]), y = as.matrix(expectancy_train[,2]),\r\n  alpha = 1,\r\n  lambda = lambda_min_MSE_LASSO\r\n)\r\n\r\n#ridge holdout\r\nmodel_ridge_holdout <- glmnet( \r\n  x = as.matrix(expectancy_train[,-c(1,2)]), y = as.matrix(expectancy_train[,2]),\r\n  alpha = 0,\r\n  lambda = lambda_min_MSE_RIDGE\r\n)\r\n\r\n#elastic cv\r\nmodel_elastic <- train(\r\n  Life.expectancy ~., data = expectancy_train[,-1], method = \"glmnet\",\r\n  preProcess = c(\"center\", \"scale\"),\r\n  trControl = train_control,\r\n  tuneLength = 10\r\n  )\r\n\r\n\r\n\r\nIn order to test which model is the best, we will be finding the RMSE’s of the models and comparing.\r\n\r\n\r\nShow code\r\n\r\nholdout.prediction <- predict(model_holdout, newdata = expectancy_test[,-2])\r\n\r\nholdout.rmse <- rmse(\r\n    preds = holdout.prediction,\r\n    actuals = expectancy_test$Life.expectancy)\r\n\r\nmodel.5CV.pred <- predict(model_5CV, newdata = expectancy_test[,-2])\r\n\r\nmodel.5CV.rmse <- rmse(\r\n    preds = model.5CV.pred,\r\n    actuals = expectancy_test$Life.expectancy)\r\n\r\nlasso.holdout.pred <- predict(model_lasso_holdout, newx = as.matrix(expectancy_test[,-c(1,2)]))\r\n\r\nlasso.cv.pred <- predict(model_lasso_cv, newx = as.matrix(expectancy_test[,-c(1,2)]))\r\n\r\nridge.holdout.pred <- predict(model_ridge_holdout, newx = as.matrix(expectancy_test[,-c(1,2)]))\r\n\r\nlasso.cv.pred <- predict(model_lasso_cv, model.matrix(Life.expectancy ~., expectancy_test)[,-c(1,2)])\r\nlasso.cv.rmse <- rmse(lasso.cv.pred,expectancy_test$Life.expectancy)\r\n\r\nridge.cv.pred <- predict(model_ridge_cv, model.matrix(Life.expectancy ~., expectancy_test)[,-c(1,2)])\r\nridge.cv.rmse <- rmse(ridge.cv.pred,expectancy_test$Life.expectancy)\r\n\r\nelastic.cv.pred <- predict(model_elastic, model.matrix(Life.expectancy ~., expectancy_test)[,-c(1,2)])\r\nelastic.cv.rmse <- rmse(elastic.cv.pred,expectancy_test$Life.expectancy)\r\n\r\nholdout.lasso.rmse <- rmse(\r\n    preds = lasso.holdout.pred,\r\n    actuals = expectancy_test$Life.expectancy)\r\n\r\nholdout.ridge.rmse <- rmse(\r\n    preds = ridge.holdout.pred,\r\n    actuals = expectancy_test$Life.expectancy)\r\n\r\nRMSE <- tibble(\r\n    Method = \"LM Hold-out\",\r\n    RMSE = holdout.rmse)\r\n\r\nRMSE <- rbind(RMSE,tibble(\r\n               Method = \"5 fold LM CV\",\r\n               RMSE = model.5CV.rmse))\r\n\r\nRMSE <- rbind(RMSE,tibble(\r\n               Method = \"LASSO CV\",\r\n               RMSE = lasso.cv.rmse))\r\n\r\nRMSE <- rbind(RMSE,tibble(\r\n               Method = \"RIDGE CV\",\r\n               RMSE = ridge.cv.rmse))\r\n\r\nRMSE <- rbind(RMSE,tibble(\r\n               Method = \"Elastic CV\",\r\n               RMSE = elastic.cv.rmse))\r\n\r\nRMSE <- rbind(RMSE, tibble(\r\n               Method = \"LASSO Hold-out\",\r\n               RMSE = holdout.lasso.rmse))\r\n\r\nRMSE <- rbind(RMSE, tibble(\r\n               Method = \"RIDGE Hold-out\",\r\n               RMSE = holdout.ridge.rmse))\r\nRMSE  %>% kbl() %>% \r\n  kable_paper(\"hover\", full_width=0)\r\n\r\n\r\n\r\nMethod\r\n\r\n\r\nRMSE\r\n\r\n\r\nLM Hold-out\r\n\r\n\r\n3.919471\r\n\r\n\r\n5 fold LM CV\r\n\r\n\r\n3.919471\r\n\r\n\r\nLASSO CV\r\n\r\n\r\n3.664512\r\n\r\n\r\nRIDGE CV\r\n\r\n\r\n3.720795\r\n\r\n\r\nElastic CV\r\n\r\n\r\n3.663028\r\n\r\n\r\nLASSO Hold-out\r\n\r\n\r\n3.667097\r\n\r\n\r\nRIDGE Hold-out\r\n\r\n\r\n3.663632\r\n\r\n\r\nWe can see that the model with the lowest RMSE is the elastic model with an RMSE of 3.663029. The coefficients for this model can be found below.\r\n\r\n\r\nShow code\r\n\r\ncoef(model_elastic$finalModel, model_elastic$bestTune$lambda) \r\n\r\n\r\n19 x 1 sparse Matrix of class \"dgCMatrix\"\r\n                                          s1\r\n(Intercept)                      69.10258900\r\nAdult.Mortality                  -2.30326426\r\ninfant.deaths                    10.27596986\r\nAlcohol                          -0.28613317\r\npercentage.expenditure            0.62442166\r\nHepatitis.B                      -0.06411130\r\nMeasles                          -0.04012417\r\nBMI                               0.66222164\r\nunder.five.deaths               -10.55426961\r\nPolio                             0.27901943\r\nTotal.expenditure                 0.14502042\r\nDiphtheria                        0.18339641\r\nHIV.AIDS                         -2.68744580\r\nGDP                               0.19137946\r\nPopulation                        0.02274985\r\nthinness..1.19.years             -0.02926623\r\nthinness.5.9.years               -0.24398017\r\nIncome.composition.of.resources   1.74121425\r\nSchooling                         2.42970613\r\n\r\n\r\n\r\nShow code\r\n\r\nmodel_elastic$bestTune %>% kbl() %>% \r\n  kable_paper(\"hover\", full_width=0)\r\n\r\n\r\n\r\n\r\n\r\nalpha\r\n\r\n\r\nlambda\r\n\r\n\r\n11\r\n\r\n\r\n0.2\r\n\r\n\r\n0.0029358\r\n\r\n\r\nThis model is a convex combination of Ridge and LASSO. The equation to understand the alpha for this elastic model is \\(\\alpha \\times LASSO + (1-\\alpha) \\times RIDGE\\), which combines the L1 and L2 norms.\r\n\r\n\r\nShow code\r\n\r\nggplot(varImp(model_elastic)) + ggtitle(\"Most important variables from elastic CV\") + labs(caption = \"Figure 10\") \r\n\r\n\r\n\r\n\r\nObservations\r\nThe most important variables we get from this are under five deaths, infant deaths, HIV/AIDS, Adult mortality, Income composition of variables, BMI and percentage expenditure.\r\nPopulation, thinness, Measles, Hepatitis B and total expenditure are less important.\r\nAlcohol and Polio are right in the middle of the variable importance graph.\r\nDiscussion\r\nResults and impacts\r\nIn our study we used variable selection methods (both forward and backward) to find that the best model is the one with 12 covariates. From there, we used those covariates to create cross-validation models with 5-fold and holdout models. Additionally, we performed LASSO and ridge and created 5-fold cross validation and hold-out models for the two. The last type of model we did was an elastic cross-validation model. We discovered by comparing RMSE that the best model is the elastic cross-validation, and thus found the coefficients for the specific model. Our findings help us see the best model in predicting life expectancy, thus also allowing us to see which factors affect life expectancy the most. Our analysis showed us that Under-five deaths, infant deaths, HIV/AIDS, Adult Mortality, Schooling and income composition of resources are the factors that affect life expectancy the most.\r\nAre the results what we expected?\r\nIt is unsurprising that a disease such as HIV/AIDS affect life expectancy. One could question why Measles was not equally as important, but a reason for this could be that there is a vaccine which is quite widespread, and so the disease is not as impactful anymore.\r\nSimilarly, infant deaths, under five deaths and adult mortality being the most important are also what we expected, as they directly involve life expectancy.\r\nOur variable selection methods suggest that Alcohol is an important aspect of our model, but the Important variables for our best model seem to suggest otherwise. We would expect Alcohol to be higher up on the important variables graph.\r\nImprovements that could be made\r\nThis study only takes into account additive models, and so, we are not able to explore the relationship between variables in the model. While we showed that the \\(C_p\\) is much higher with interactions, our model could be improved using interactions, so there is further room for improvement here.\r\nFuture questions/research\r\nOur results with this study hae allowed us to see the factors affecting life expectancy and the best way to predict life expectancy in any random country. In the future, this study could lead to questions about: - How can governments help in increasing life expectancy and what areas should they be focusing on specifically? - The data in this study predates Covid-19. How much does Covid-19 affect life expectancy and how much would it change our model? - As mentioned above, the data in this study is from 2000 - 2015. There have been multiple advancements in the medical field since then. Further, 15 years is also a long period of time. It would be interesting to explore how for different and shorter periods of time how life expectancy changes and how its changed over time.\r\nReferences\r\n\\(^{(1)}\\) World Health Organisation (WHO). (2018, February 10). Life expectancy (WHO). Kaggle. Retrieved November 6, 2021, from https://www.kaggle.com/kumarajarshi/life-expectancy-who.\r\n\\(^{(2)}\\) Tier 1-life expectancy and wellbeing-1.19 life expectancy at birth. Department of Health | Tier 1-Life expectancy and wellbeing-1.19 Life expectancy at birth. (n.d.). Retrieved November 6, 2021, from https://www1.health.gov.au/internet/publications/publishing.nsf/Content/oatsih-hpf-2012-toc~tier1~life-exp-wellb~119.\r\nNational Progress Report 2025 Goal: Reduce HBV Deaths. (2021, June 03). Retrieved from https://www.cdc.gov/hepatitis/policy/NPR/2021/NationalProgressReport-HepB-ReduceDeaths.htm\r\nClinical Features of Diphtheria. (2020, May 26). Retrieved from https://www.cdc.gov/diphtheria/clinicians.html\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-17-life-expectancy-analysis/life-expectancy-analysis_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-12-19T00:12:16-08:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-19-card-default-prediction/",
    "title": "Card Default Visualization and Prediction",
    "description": "The data set we are using is an aggregate of credit card data from various individuals in Taiwan that includes variables such as education, age, whether or not they defaulted on a payment and others. Default on payment means that an individual did not pay their credit card bills. Our question for this data analytics project is: \"How accurately can Education, and Age predict the likelihood of an individual defaulting on their next credit card payment?\"",
    "author": [
      {
        "name": "Blaise Appolinary",
        "url": "https://ca.linkedin.com/in/blaise-appolinary-038a42179"
      },
      {
        "name": "Amogh Joshi",
        "url": {}
      },
      {
        "name": "Derrick Cheng",
        "url": {}
      },
      {
        "name": "Michael Hoefert",
        "url": {}
      }
    ],
    "date": "2021-12-17",
    "categories": [],
    "contents": "\r\n\r\nContents\r\nMethods\r\nExploratory Data Analysis\r\nCorrelation network plot\r\n\r\nModelling\r\nTraining Vs Testing split\r\nSummary Statistics in Training set\r\n\r\nTraining\r\nFinding the best K for the model\r\n\r\nDiscussion\r\nRefferences\r\n\r\n Why did we choose these predictor variables over the numerous other variables in the data set? Our first objective was to eliminate those variables we felt did not contribute at all to the probability of default payments, such as sex. Next, we did some external research and found that of the remaining variables that we had access to in the dataset, age and education matched the best predictors of default probability from our external research (T. M. Alam et al., 2020). Thus, we will build our classification model on these two predictor models to determine the accuracy to which we can predict default probability. \r\n The dataset we will be using has 25 variables and 30002 records. The data set includes the variables we mentioned above and others such as the bill amounts each client has as well as the payments each client has made towards those bill amounts. Education is classified with numbers 1 (Graduate School), 2 (University), 3 (High School), and 4 (Others). Age is simply the numeric value of the individual’s age. The default payment variable is represented by 1’s (Yes, did default) and 0’s (No, did not default). \r\n We are interested in this question since credit card default is a real problem in the world, with credit card delinquency rates increasing by 0.180% year on year from 2017 to 2018, in Taiwan (CEIC, 2018). Using the models we create, we can determine the risk of lending credit to an individual with a high risk of default and therefore protect the creditors. \r\nMethods\r\nWe will be including the following columns: Education, Age and Default payment next month. We will conduct our data analysis using classification. Our classification will be done by creating a test set and a training set. Following this, we will begin to create a model that will tune the K nearest neighbors value we should use and a recipe that uses the straight line (“rectangular”) method to predict classifications. Following this we will fold the training set (most likely 10 times) to allow for a relatively accurate prediction of the K nearest neighbors value we should use. Using this, we will move forward with applying our model to our test set and predicting the classifications. \r\n We will create a bar chart that will depict the proportion of how many classifications were correctly predicted next to another bar that depicts the proportion of classifications that were incorrectly predicted. \r\nExpected outcomes and significance:   We expect to find that there is a correlation between the variables education, age and the classification of whether the individual defaulted. We expect to find that we will be able to get a fairly accurate accuracy. We define fairly accurate as an accuracy above 70%. Findings like this can allow credit card companies to apply higher late/default rates to customers who are more likely to default on payments based on our model and classification. Future questions would be: \r\n\r\n How can we make our predictions more accurate? \r\n\r\n\r\n Is it ethical to change rates/interest on individuals who have been classified by a model to be more likely to default? \r\n\r\nExploratory Data Analysis\r\n\r\n\r\nShow code\r\n\r\ncredit<- read_excel(\"credit_card.xls\", skip = 1) %>% rename(default_payment_next_month= \"default payment next month\")\r\ncredit %>% head() %>%  rmarkdown::paged_table()\r\n\r\n\r\n\r\n\r\n{\"columns\":[{\"label\":[\"ID\"],\"name\":[1],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"LIMIT_BAL\"],\"name\":[2],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"SEX\"],\"name\":[3],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"EDUCATION\"],\"name\":[4],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"MARRIAGE\"],\"name\":[5],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"AGE\"],\"name\":[6],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_0\"],\"name\":[7],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_2\"],\"name\":[8],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_3\"],\"name\":[9],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_4\"],\"name\":[10],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_5\"],\"name\":[11],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_6\"],\"name\":[12],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BILL_AMT1\"],\"name\":[13],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BILL_AMT2\"],\"name\":[14],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BILL_AMT3\"],\"name\":[15],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BILL_AMT4\"],\"name\":[16],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BILL_AMT5\"],\"name\":[17],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"BILL_AMT6\"],\"name\":[18],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_AMT1\"],\"name\":[19],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_AMT2\"],\"name\":[20],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_AMT3\"],\"name\":[21],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_AMT4\"],\"name\":[22],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_AMT5\"],\"name\":[23],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"PAY_AMT6\"],\"name\":[24],\"type\":[\"dbl\"],\"align\":[\"right\"]},{\"label\":[\"default_payment_next_month\"],\"name\":[25],\"type\":[\"dbl\"],\"align\":[\"right\"]}],\"data\":[{\"1\":\"1\",\"2\":\"20000\",\"3\":\"2\",\"4\":\"2\",\"5\":\"1\",\"6\":\"24\",\"7\":\"2\",\"8\":\"2\",\"9\":\"-1\",\"10\":\"-1\",\"11\":\"-2\",\"12\":\"-2\",\"13\":\"3913\",\"14\":\"3102\",\"15\":\"689\",\"16\":\"0\",\"17\":\"0\",\"18\":\"0\",\"19\":\"0\",\"20\":\"689\",\"21\":\"0\",\"22\":\"0\",\"23\":\"0\",\"24\":\"0\",\"25\":\"1\"},{\"1\":\"2\",\"2\":\"120000\",\"3\":\"2\",\"4\":\"2\",\"5\":\"2\",\"6\":\"26\",\"7\":\"-1\",\"8\":\"2\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"2\",\"13\":\"2682\",\"14\":\"1725\",\"15\":\"2682\",\"16\":\"3272\",\"17\":\"3455\",\"18\":\"3261\",\"19\":\"0\",\"20\":\"1000\",\"21\":\"1000\",\"22\":\"1000\",\"23\":\"0\",\"24\":\"2000\",\"25\":\"1\"},{\"1\":\"3\",\"2\":\"90000\",\"3\":\"2\",\"4\":\"2\",\"5\":\"2\",\"6\":\"34\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"29239\",\"14\":\"14027\",\"15\":\"13559\",\"16\":\"14331\",\"17\":\"14948\",\"18\":\"15549\",\"19\":\"1518\",\"20\":\"1500\",\"21\":\"1000\",\"22\":\"1000\",\"23\":\"1000\",\"24\":\"5000\",\"25\":\"0\"},{\"1\":\"4\",\"2\":\"50000\",\"3\":\"2\",\"4\":\"2\",\"5\":\"1\",\"6\":\"37\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"46990\",\"14\":\"48233\",\"15\":\"49291\",\"16\":\"28314\",\"17\":\"28959\",\"18\":\"29547\",\"19\":\"2000\",\"20\":\"2019\",\"21\":\"1200\",\"22\":\"1100\",\"23\":\"1069\",\"24\":\"1000\",\"25\":\"0\"},{\"1\":\"5\",\"2\":\"50000\",\"3\":\"1\",\"4\":\"2\",\"5\":\"1\",\"6\":\"57\",\"7\":\"-1\",\"8\":\"0\",\"9\":\"-1\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"8617\",\"14\":\"5670\",\"15\":\"35835\",\"16\":\"20940\",\"17\":\"19146\",\"18\":\"19131\",\"19\":\"2000\",\"20\":\"36681\",\"21\":\"10000\",\"22\":\"9000\",\"23\":\"689\",\"24\":\"679\",\"25\":\"0\"},{\"1\":\"6\",\"2\":\"50000\",\"3\":\"1\",\"4\":\"1\",\"5\":\"2\",\"6\":\"37\",\"7\":\"0\",\"8\":\"0\",\"9\":\"0\",\"10\":\"0\",\"11\":\"0\",\"12\":\"0\",\"13\":\"64400\",\"14\":\"57069\",\"15\":\"57608\",\"16\":\"19394\",\"17\":\"19619\",\"18\":\"20024\",\"19\":\"2500\",\"20\":\"1815\",\"21\":\"657\",\"22\":\"1000\",\"23\":\"1000\",\"24\":\"800\",\"25\":\"0\"}],\"options\":{\"columns\":{\"min\":{},\"max\":[10]},\"rows\":{\"min\":[10],\"max\":[10]},\"pages\":{}}}\r\n  \r\n\r\nCorrelation network plot\r\n\r\n\r\nShow code\r\n\r\nnew_data <- credit %>% select(EDUCATION, AGE, default_payment_next_month)\r\ncor <- corrr::correlate(credit %>% select(-ID)) %>% \r\n  stretch()\r\ngraph_from_cor <- cor %>%  filter(abs(r) > 0.3) %>%\r\n  graph_from_data_frame(directed = FALSE)\r\nggraph(graph_from_cor) +\r\n  geom_edge_link()+\r\n  geom_node_point(color = \"brown\", size = 5) +\r\n  geom_node_text(aes(label = name), repel = TRUE) +\r\n  theme_graph() +\r\n  labs(title = \"Correlations Network Plot\")\r\n\r\n\r\n\r\n\r\nFigure 1: Correlation plot between different variables\r\n\r\n\r\n\r\nModelling\r\nTraining Vs Testing split\r\n Once we read the file into our Jupyter Notebook we made sure that the data was clean. To make sure it was clean we double checked that there were no N/A values, there were none. We identified our variable that would be our classifier for our prediction and converted it into a factor to ensure we would be able to classify it. The next step was to split the large data set into our training and test set. We did this because we want to make sure that we do not use the testing dataset at all until the very end once we have created our models, recipes and found the most accurate K nearest neighbors value. We split the data sets as 75% training and 25% training. We set the strata equal to our classification variable, “default_payment_next_month”. Next, we finalized the split of the data into two different data sets; credit_train and credit_test. \r\n\r\n\r\nShow code\r\n\r\nset.seed(1234)\r\ntidied_credit_card <- credit %>% \r\n                    select(EDUCATION, AGE, default_payment_next_month) %>%\r\n                    mutate(default_payment_next_month = as_factor(default_payment_next_month)) %>%\r\n                    filter(EDUCATION == c(1,2,3,4))\r\ncredit_split <- initial_split(tidied_credit_card, prop = 0.75, strata = default_payment_next_month)\r\ncredit_train <- training(credit_split)\r\ncredit_test <- testing(credit_split)\r\ncredit_train %>% head %>% kbl() %>% \r\n  kable_paper(\"hover\",full_width=F)\r\n\r\n\r\n\r\nEDUCATION\r\n\r\n\r\nAGE\r\n\r\n\r\ndefault_payment_next_month\r\n\r\n\r\n1\r\n\r\n\r\n23\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n32\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n54\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n22\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n30\r\n\r\n\r\n0\r\n\r\n\r\n2\r\n\r\n\r\n39\r\n\r\n\r\n0\r\n\r\n\r\nSummary Statistics in Training set\r\n\r\n\r\nShow code\r\n\r\nskim_without_charts(credit_train)\r\n\r\n\r\n\r\nTable 1: Data summary\r\n\r\n\r\n\r\n\r\n\r\n\r\nName\r\n\r\n\r\ncredit_train\r\n\r\n\r\nNumber of rows\r\n\r\n\r\n5523\r\n\r\n\r\nNumber of columns\r\n\r\n\r\n3\r\n\r\n\r\n_______________________\r\n\r\n\r\n\r\n\r\nColumn type frequency:\r\n\r\n\r\n\r\n\r\nfactor\r\n\r\n\r\n1\r\n\r\n\r\nnumeric\r\n\r\n\r\n2\r\n\r\n\r\n________________________\r\n\r\n\r\n\r\n\r\nGroup variables\r\n\r\n\r\nNone\r\n\r\nVariable type: factor\r\n\r\nskim_variable\r\n\r\n\r\nn_missing\r\n\r\n\r\ncomplete_rate\r\n\r\n\r\nordered\r\n\r\n\r\nn_unique\r\n\r\n\r\ntop_counts\r\n\r\n\r\ndefault_payment_next_month\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\nFALSE\r\n\r\n\r\n2\r\n\r\n\r\n0: 4305, 1: 1218\r\n\r\nVariable type: numeric\r\n\r\nskim_variable\r\n\r\n\r\nn_missing\r\n\r\n\r\ncomplete_rate\r\n\r\n\r\nmean\r\n\r\n\r\nsd\r\n\r\n\r\np0\r\n\r\n\r\np25\r\n\r\n\r\np50\r\n\r\n\r\np75\r\n\r\n\r\np100\r\n\r\n\r\nEDUCATION\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n1.81\r\n\r\n\r\n0.71\r\n\r\n\r\n1\r\n\r\n\r\n1\r\n\r\n\r\n2\r\n\r\n\r\n2\r\n\r\n\r\n4\r\n\r\n\r\nAGE\r\n\r\n\r\n0\r\n\r\n\r\n1\r\n\r\n\r\n35.47\r\n\r\n\r\n9.16\r\n\r\n\r\n21\r\n\r\n\r\n28\r\n\r\n\r\n34\r\n\r\n\r\n42\r\n\r\n\r\n75\r\n\r\n\r\n In order to get a sense for the training dataset and to understand it more thoroughly, we did some preliminary data analysis. We firstly noticed that the dataset had more choices for education than what was stated on the website, so rather than there just being 4 choices, there were 6. Therefore, since we did not know what the extra choices referred to (and with the help of a TA who advised us to do the same), we filtered out said choices. The impact of this filtering was very minor since it only affected about 20 data samples. \r\n\r\n\r\nShow code\r\n\r\ncredit_summarize <- credit_train %>%\r\n                    group_by(EDUCATION, AGE, default_payment_next_month) %>%\r\n                    summarize(n = n()) \r\ncredit_summarize %>% data.table()\r\n\r\n\r\n     EDUCATION AGE default_payment_next_month  n\r\n  1:         1  21                          0  2\r\n  2:         1  22                          0 18\r\n  3:         1  22                          1  6\r\n  4:         1  23                          0 47\r\n  5:         1  23                          1 12\r\n ---                                            \r\n281:         4  44                          1  1\r\n282:         4  48                          0  1\r\n283:         4  49                          0  1\r\n284:         4  50                          0  1\r\n285:         4  52                          0  1\r\n\r\n Next, we graphed the filtered data to understand how the data was balanced. We wanted to see if there was an obvious correlation between the variables, and as such, grouped the observations and graphed them using a barchart. We did separate graphs for education and age in order to isolate so we can gain a better understanding of how the variable impacts risk of credit default. \r\n\r\n\r\nShow code\r\n\r\ncredit_education_graph <- ggplot(credit_summarize, aes(x = EDUCATION, y = n, fill = default_payment_next_month)) +\r\n                geom_bar(stat = \"identity\", position = \"fill\") +\r\n                labs(x = \"Education\", y = \"Default Payment\", title = \"Education VS Default Payment\", fill = \"Defaulted on Payment\") +\r\n                theme(text = element_text(size=20))\r\ncredit_education_graph\r\n\r\n\r\n\r\n\r\nFigure 2:  Distrubution of Default Payment for Education. This graph shows the distribution between each education level and the number of default payments 1 (Yes) and 0 (No) in the training data set. The graph shows that Education levels 2 and 3 have the highest default rate.\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nset.seed(1234)\r\ncredit_sex_graph <- ggplot(credit_summarize, aes(x = AGE, y = n, fill = default_payment_next_month)) +\r\n                geom_bar(stat = \"identity\", position = \"fill\") +\r\n                labs(x = \"Age\", y = \"Default Payment\", title = \"Default Payment vs Age\", fill = \"Defaulted on Payment\") +\r\n                theme(text = element_text(size=20))\r\ncredit_sex_graph\r\n\r\n\r\n\r\n\r\nFigure 3: Figure 2: Distrubution of Default Payment for Age. This graph shows the distribution between each age and the number of default payments 1 (Yes) and 0 (No) in the training data set. The graph shows that older ages around 60+ have a higher rate of default payment, which could be attributed to smaller amounts of data points from the age range.\r\n\r\n\r\n\r\nTraining\r\nThe next step of our analysis was building our recipe, model and workflow. For our model we used the rectangular method of prediction and tuned our neighbors function. We used tune instead of placing an arbitrary number because we are trying to determine, with the tune function, what the most accurate K nearest neighbors value is. Next, for our recipe, we identified the variable we are trying to predict (default_payment_next_month) and our predictor variables (EDUCATION and AGE). We are building the recipe on our training data set, because as we previously mentioned, we do not want to touch the test set at all until the final analysis/prediction/classification. For our recipe, we also scaled and centered all of our predictor variables. Following the recipe, we created our vfold function. We created the vfold function so that we can get a better K nearest neighbors value estimate by splitting the data multiple ways and averaging the results. We decided to fold the data set 5 times, anymore than this and we found we were waiting for significant amounts of time for the Jupyter Notebook to run. To effectively use our vfold function, we decided to create a tible with the sequences 1 through 200 for K nearest values 1 through 200. We will use this in our workflow to find the most accurate K nearest neighbors value. Lastly, we were putting the model, recipe and vfold functions all together into our workflow. We added the recipe, then the model. Following this, we added a tune_grid function which included our vfold function as well as the grid tibble we previously mentioned. To conclude our workflow, we are collecting the metrics with; collect_metrics().\r\n\r\n\r\nShow code\r\n\r\nset.seed(1234)\r\nknn_spec <- nearest_neighbor(weight_func = \"rectangular\", neighbors = tune()) %>%\r\n      set_engine(\"kknn\") %>%\r\n      set_mode(\"classification\")\r\ncredit_recipe <- recipe(default_payment_next_month ~ EDUCATION + AGE, data = credit_train) %>%\r\n                step_scale(all_predictors()) %>%\r\n                step_center(all_predictors())\r\ncredit_vfold <- vfold_cv(credit_train, v = 5, strata = default_payment_next_month)\r\ngridvals = tibble(neighbors = seq(1,50))\r\n\r\ncredit_fit <- workflow() %>%\r\n      add_recipe(credit_recipe) %>%\r\n      add_model(knn_spec) %>%\r\n      tune_grid(resamples = credit_vfold, grid = 100) %>%\r\n        collect_metrics()\r\ncredit_fit %>% data.table()\r\n\r\n\r\n    neighbors  .metric .estimator      mean n      std_err\r\n 1:         1 accuracy     binary 0.7778381 5 0.0010189829\r\n 2:         1  roc_auc     binary 0.5013076 5 0.0014980903\r\n 3:         2 accuracy     binary 0.7785622 5 0.0004796293\r\n 4:         2  roc_auc     binary 0.5011568 5 0.0015609007\r\n 5:         3 accuracy     binary 0.7782004 5 0.0011382331\r\n 6:         3  roc_auc     binary 0.5031854 5 0.0005344724\r\n 7:         4 accuracy     binary 0.7780192 5 0.0011214818\r\n 8:         4  roc_auc     binary 0.5044075 5 0.0015017582\r\n 9:         5 accuracy     binary 0.7771139 5 0.0010459038\r\n10:         5  roc_auc     binary 0.5061534 5 0.0012881664\r\n11:         6 accuracy     binary 0.7780191 5 0.0010543830\r\n12:         6  roc_auc     binary 0.5039148 5 0.0010771440\r\n13:         7 accuracy     binary 0.7772949 5 0.0010336610\r\n14:         7  roc_auc     binary 0.5049822 5 0.0039082790\r\n15:         8 accuracy     binary 0.7778384 5 0.0007697558\r\n16:         8  roc_auc     binary 0.5031737 5 0.0016971696\r\n17:         9 accuracy     binary 0.7782000 5 0.0007712707\r\n18:         9  roc_auc     binary 0.5020524 5 0.0026117206\r\n19:        10 accuracy     binary 0.7782000 5 0.0007712707\r\n20:        10  roc_auc     binary 0.5042900 5 0.0018335397\r\n21:        11 accuracy     binary 0.7787437 5 0.0005887663\r\n22:        11  roc_auc     binary 0.5074889 5 0.0043046895\r\n23:        12 accuracy     binary 0.7789247 5 0.0005055621\r\n24:        12  roc_auc     binary 0.5098753 5 0.0028675090\r\n25:        13 accuracy     binary 0.7791057 5 0.0002574148\r\n26:        13  roc_auc     binary 0.5093868 5 0.0040229852\r\n27:        14 accuracy     binary 0.7791057 5 0.0002574148\r\n28:        14  roc_auc     binary 0.5106601 5 0.0029876314\r\n29:        15 accuracy     binary 0.7794678 5 0.0001728811\r\n30:        15  roc_auc     binary 0.5136957 5 0.0031531812\r\n    neighbors  .metric .estimator      mean n      std_err\r\n                  .config\r\n 1: Preprocessor1_Model01\r\n 2: Preprocessor1_Model01\r\n 3: Preprocessor1_Model02\r\n 4: Preprocessor1_Model02\r\n 5: Preprocessor1_Model03\r\n 6: Preprocessor1_Model03\r\n 7: Preprocessor1_Model04\r\n 8: Preprocessor1_Model04\r\n 9: Preprocessor1_Model05\r\n10: Preprocessor1_Model05\r\n11: Preprocessor1_Model06\r\n12: Preprocessor1_Model06\r\n13: Preprocessor1_Model07\r\n14: Preprocessor1_Model07\r\n15: Preprocessor1_Model08\r\n16: Preprocessor1_Model08\r\n17: Preprocessor1_Model09\r\n18: Preprocessor1_Model09\r\n19: Preprocessor1_Model10\r\n20: Preprocessor1_Model10\r\n21: Preprocessor1_Model11\r\n22: Preprocessor1_Model11\r\n23: Preprocessor1_Model12\r\n24: Preprocessor1_Model12\r\n25: Preprocessor1_Model13\r\n26: Preprocessor1_Model13\r\n27: Preprocessor1_Model14\r\n28: Preprocessor1_Model14\r\n29: Preprocessor1_Model15\r\n30: Preprocessor1_Model15\r\n                  .config\r\n\r\nFinding the best K for the model\r\nAfter graphing our findings to find the best K value, we decided on the best K as 15. We chose the K value with the highest accuracy.\r\n\r\n\r\nShow code\r\n\r\nset.seed(1234)\r\naccuracies <- credit_fit %>% \r\n      filter(.metric == \"accuracy\")\r\n\r\naccuracy_versus_k <- ggplot(accuracies, aes(x = neighbors, y = mean))+\r\n      geom_point() +\r\n      geom_line() +\r\n      labs(x = \"Number of Neighbors\", y = \"Accuracy Estimate\", title = \"K Vs Accuracy\") +\r\n      scale_x_continuous(breaks = seq(0, 20, by = 1)) +\r\n        theme(text = element_text(size=20))\r\naccuracy_versus_k\r\n\r\n\r\n\r\n\r\nFigure 4: The graph shows that the K with the highest accuracy estimate is 15, which is the K we chose. Although we tuned with grid = 100, the K values only went up to 15. We asked a TA on how to fix this, but they didn’t know so we stuck with the choice of K = 15.\r\n\r\n\r\n\r\n Following this, we took our new K value and built out our new model spec with the K value set to 15. We used the same recipe as before and then added the recipe and new model to a workflow and fit the training data. Next, we began predicting our test set with this final model and binded the columns so that we would be able to see the predicted class and the true class of the test data set. Next we made a confusion matrix which included the true classifier values as well as the predicted classifier values from our dataset. With these predictions we were able to predict the accuracy of our model which was 77.9%. \r\n\r\n\r\nShow code\r\n\r\nset.seed(1234)\r\nknn_final <- nearest_neighbor(weight_func = \"rectangular\", neighbors = 15) %>%\r\n      set_engine(\"kknn\") %>%\r\n      set_mode(\"classification\")\r\ncredit_final = workflow() %>%\r\n      add_recipe(credit_recipe) %>%\r\n      add_model(knn_final) %>%\r\n      fit(data = credit_train)\r\n\r\n\r\n\r\n\r\n\r\nShow code\r\n\r\nset.seed(1234)\r\ncredit_predictions = predict(credit_final, credit_test) %>%\r\n      bind_cols(credit_test)\r\ncredit_metrics <- credit_predictions %>%\r\n        metrics(truth = default_payment_next_month, estimate = .pred_class) \r\ncredit_conf_mat <- credit_predictions %>%\r\n        conf_mat(truth = default_payment_next_month, estimate = .pred_class) \r\n#credit_predictions\r\n#credit_metrics\r\ncredit_conf_mat \r\n\r\n\r\n          Truth\r\nPrediction    0    1\r\n         0 1434  406\r\n         1    2    0\r\n\r\nAccuracy\r\n\r\n\r\nShow code\r\n\r\naccuracy <- slice(credit_metrics, 1) %>% pull(3)\r\naccuracy\r\n\r\n\r\n[1] 0.7785016\r\n\r\nMetric confusion matrix\r\nThe prediction, metrics, and confusion matrix’s results on the testing set, producing an accuracy of about 78%.\r\n\r\n\r\nShow code\r\n\r\nctable <- as.table(matrix(c(1436, 406, 0, 0), nrow = 2, byrow = TRUE))\r\nfourfoldplot(ctable, color = c(\"#CC6666\", \"#99CC99\"),\r\n             conf.level = 0, margin = 1, main = \"Confusion Matrix\")\r\n\r\n\r\n\r\n\r\nFigure 5: Confusion Matrix Graph. This graph shows the finding of our confusion matrix, which proportionally shows correctly classified vs incorrectly classified.\r\n\r\n\r\n\r\nAccuracy Pie Chart\r\n\r\n\r\nShow code\r\n\r\ndf <- data.frame(\r\n  group = c(\"Percent Correct\", \"Percent Incorrect\"),\r\n  value = c(accuracy * 100, 100 - (accuracy * 100))\r\n  )\r\nbp<- ggplot(df, aes(x=\"\", y=value, fill=group))+\r\ngeom_bar(width = 1, stat = \"identity\") +\r\nggtitle(\"Accuracy Pie Chart\") +\r\nlabs(fill = \"Percent Correct and Incorrect\")\r\n\r\npie <- bp + coord_polar(\"y\", start=0)\r\npie <- pie + scale_fill_manual(values=c(\"#99CC99\", \"#CC6666\")) + theme(text = element_text(size=20))\r\npie\r\n\r\n\r\n\r\n\r\nFigure 6:  Summary Pie Chart Of Our Finding. This pie chart summarizes the accuracy of our findings, which is correct about 78% of the time.\r\n\r\n\r\n\r\nDiscussion\r\n​  Our final accuracy from our tuned model was 77.9%. From the final graphs that we plotted, we found that our classification analysis had a fairly high accuracy with the predictor values that we chose. Although the accuracy is fairly high, there is most definitely room for improvement. If you reference the pie chart, on average, our model correctly predicts the likelihood of credit card default approximately 3 out 4 times. With this fairly high accuracy, it is safe to say that the education level an individual has and their age does play a role in their likelihood to default.  ​  The accuracy we achieved from our model exceeded what was predicted; as stated in our introduction, we thought that the model would be about 70% accurate, however, it was closer to a 77.9% accuracy rate. Since the accuracy was higher than 50% (based on randomly selecting credit risk), it can be confidently stated that there is a correlation between an individual’s education and age, and their credit default risk.  ​  The impact of the findings within this report are of great consequence to the financial industry. Given that our model was able to predict credit card default fairly accurately, banks and other financial institutions can both use our model, and build on it to improve it, to further mitigate their risk. As lending money to individuals has its inherent risk, it is absolutely imperative that these financial institutions mitigate as much risk as possible to ensure secure cash flows and revenue from their credit card business.  ​  Due to the onset of COVID-19, the economic realities are much different than in 2016, the year in which the data set was donated. Due to this, a potential further question that would be very interesting to investigate is if the created model is still relevant today? Since unemployment rates at the height of lockdowns was about 4.1%, the highest it has been in over 6 years (Reuters, 2020). While we have not shown that credit card default and unemployment are correlated, the two variables have been linked in other articles (Bai, 2016). As such, investigating if the model holds in times of economic depression would be a question to investigate in the future. \r\n However, one of the questions that we feel is most critical in investigating is whether or not using predictive modelling to determine the risk of an individual in defaulting is ethical. The availability of credit has been linked to increased economic output, and the ability to achieve a higher quality of life (Proctor & Anand, 2017, 322-346). As such, if the predictive model is not entirely accurate or has inherent biases present (such as giving different risk scores for different races, sex, etc.), it could lead to a negative feedback loop for individuals already experiencing economic hardship with no escape from poverty through credit. As such, more research needs to be conducted into the ethical realities of using a model to judge a person’s credit risk.\r\nRefferences\r\n​  Alam, Talha Mahboob, et al. “An Investigation of Credit Card Default Prediction in the Imbalanced Datasets.” IEEE Xplore, IEEE, 26 Oct. 2020, ieeexplore.ieee.org/document/9239944?fbclid=IwAR3zMjKdc4EdLV5A4r5RRW_BFy1VnOY9QQewm8qRdsYTlPFBCMurTpU6DA0.  ​  Bai, H. (2016). Unemployment and Credit Risk. Supply and Demand eJournal. https://www.semanticscholar.org/paper/Unemployment-and-Credit-Risk-Bai/375a7d56701fe8ec018feb308669e7baa9322cbd#paper-header CEIC. (2018, May). Taiwan: Credit CARD STATISTICS: CEIC. Retrieved March 12, 2021, from https://www.ceicdata.com/en/taiwan/credit-card-statistics  ​  Proctor, J., & Anand, P. (2017, September 6). Is credit associated with a higher quality of life? A capability approach. Progress in Development Studies, 17(4), 322 - 346. https://journals-sagepub-com.ezproxy.library.ubc.ca/doi/full/10.1177/1464993417716359?utm_source=summon&utm_medium=discovery-provider  ​  Reuters Staff. (2020, May 22). Taiwan jobless rate spikes to six year high on virus impact. Reuters. https://www.reuters.com/article/healthcoronavirus-taiwan-unemployment-idUSL4N2C90Z4 \r\n\r\n\r\n\r\n",
    "preview": "posts/2021-12-19-card-default-prediction/card-default-prediction_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-12-22T21:43:53-08:00",
    "input_file": {}
  }
]
